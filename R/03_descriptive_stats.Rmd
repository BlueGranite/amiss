---
title: "Descriptive statistics"
output: html_notebook
---

```{r}
library(magrittr)
library(ggplot2)
library(gridExtra)
library(ggcorrplot)

source("../R/visualizations.R")
source("../R/feature_definitions.R")

training_set <- read.csv("../preprocessed_training_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../training_outcomes.csv", row.names = 1)[,1]

stopifnot(row.names(training_set) == row.names(outcome))

features <- colnames(training_set)
```

## Correlations

Plot correlation matrices of missingness indicators against missingness indicators, observed values against observed values, and missingness indicators against observed values.
```{r fig.height=10, fig.width=10}
positive_data <- training_set[outcome == 1.0, ]
negative_data <- training_set[outcome == 0.0, ]

# Missingness indicator correlations
plot_missingness_correlations(training_set, numeric_features, "Missingness indicator correlations")
plot_missingness_correlations(positive_data, numeric_features, "Missingness indicator correlations (positive-labeled)")
plot_missingness_correlations(negative_data, numeric_features, "Missingness indicator correlations (negative-labeled)")

# Observed value correlations
plot_observed_correlations(training_set, numeric_features, "Correlations of observed values")
plot_observed_correlations(positive_data, numeric_features, "Correlations of observed values (positive-labeled)")
plot_observed_correlations(negative_data, numeric_features, "Correlations of observed values (negative-labeled)")

# Missingness vs. observed correlations
plot_missingness_vs_observed_correlations(training_set, numeric_features, "Missingness correlations vs. observed values")
plot_missingness_vs_observed_correlations(positive_data, numeric_features, "Missingness correlations vs. observed values (positive-labeled)")
plot_missingness_vs_observed_correlations(negative_data, numeric_features, "Missingness correlations vs. observed values (negative-labeled)")
```

## Feature value distributions

Next, plot distributions of each feature. Are they normal or linear?
```{r}
feature_distribution_plots <- lapply(numeric_features,
                                     function(column) {
                                       ggplot2::quickplot(
                                         na.omit(training_set[,column]),
                                         main = column,
                                         xlab = "",
                                         bins = 30
                                       )
                                     })

marrangeGrob(
  ncol = 2, nrow = 3,
  grobs = feature_distribution_plots
)
```
They are not, and thus it might be worth considering data transformations. In the case of random forest, however, monotone transformations should have no effect.

## Categorical level occurence counts

Print (one-dimensional) contingency tables, i.e. occurence counts of each level of categorical variables.
```{r}
for (cat_feat in categorical_features) {
  table(training_set[, cat_feat, drop = FALSE], dnn = cat_feat, useNA = "always") %>% as.data.frame %>% print
  cat("\n")
}
```
Looking at `Consequence.x` is redundant as it was already displayed earlier, but looking at `LRT_pred` we can see a troublingly low number of observations of level `U`.

## Heatmap of feature missingness against consequence

It is likely that missing values are more or less common in some variables depending on the predicted consequence. This can be visualized by a heatmap:
```{r}
missing_value_sum_per_consequence <- lapply(training_set[, features, drop = FALSE],
                                            function(column) {
                                              sapply(
                                                split(is.na(column), training_set$Consequence.x),
                                                sum
                                              )
                                            })
missing_value_sum_per_consequence %<>% data.frame %>% as.matrix
heatmap(missing_value_sum_per_consequence)
```
Stop-gained and non-synonymous variants have much less missingness in certain variables (as expected), and missingness rates are somewhat constant over different consequences in epigenetics variables.

## Compute number of observed missingness patterns

The number of observed missingness patterns over the data influences the feasibility of using reduced-feature models; in the strict form, you need exactly as many models as you have missingness patterns.
```{r}
missingness_patterns <- training_set[, c(numeric_features, categorical_features)] %>% is.na
unique_missingness_patterns <- missingness_patterns %>% unique
num_missingness_patterns <- unique_missingness_patterns %>% nrow
print(paste(num_missingness_patterns, "out of", 2^length(c(numeric_features, categorical_features)), "possible missingness patterns."))
```
The number is not as bad as it could be, but still large.

The most important thing is of course whether the different patterns contain sufficiently many samples to train a model on each one.
```{r}
missingness_pattern_factor <- apply(missingness_patterns, MARGIN = 1, function(x) paste0(as.integer(x), collapse = "")) %>% factor
rows_per_missingness_pattern <- table(missingness_pattern_factor)
rows_per_missingness_pattern <- rows_per_missingness_pattern %>% as.data.frame
rows_per_missingness_pattern[order(rows_per_missingness_pattern$Freq, decreasing = TRUE),]
```
Looking at it this way, it seems that most patterns would not have enough data to train a useful classifier.

However, the above computation counts only exact occurrences of missing patterns, i.e. rows where the missingness pattern for that row exactly matches the missingness pattern in question.

In truth, when training a classifier for a missingness pattern `m`, we can use any row for which we have *at least* those features indicated present in pattern `m`.
For example, for a pattern `00110`, we can use also rows with pattern `00010`, `00100` or `00000`.
```{r}
# We form a matrix where a value in cell (x,y) will be `TRUE` iff every available value of
# missingness pattern y is also available in missingness pattern x, and thus the examples
# that fit pattern x could be used to train a model on pattern y.
#
# Note that in the output, the x's are rows, and y's are columns.
pattern_intersects <- apply(unique_missingness_patterns, MARGIN = 1, function(row_x) {
  apply(unique_missingness_patterns, MARGIN = 1, function(row_y) {
    # `row_x` is the indicator vector of *missing* values of `x`,
    # and `!row_y` is the indicator vector of *available* values of `y`.
    # Since we are trying to apply rows with pattern `x` for a model for pattern `y`,
    # there musn't be any value for which `y` is available and `x` is not.
    !any((row_x & !row_y))
  })
})
unique_missingness_patterns_strings <- unique_missingness_patterns %>% apply(MARGIN = 1, FUN = . %>% as.integer %>% paste0(collapse = ""))
row.names(pattern_intersects) <- unique_missingness_patterns_strings
colnames(pattern_intersects) <- unique_missingness_patterns_strings

# Since we're thinking about training a model for y, we must sum the over the columns.
pattern_counts <- apply(pattern_intersects, MARGIN = 2, function(x) {
  # The counts of examples fitting a pattern is read from the previously computed table.
  sum(rows_per_missingness_pattern[x, "Freq"])
})
pattern_counts <- data.frame(missingness_pattern = factor(names(pattern_counts)), Freq = pattern_counts, row.names = NULL)
pattern_counts[order(pattern_counts$Freq, decreasing = TRUE), , drop = FALSE]
```
Looking at this table, the situation is less bad.

```{r}
sum(pattern_counts$Freq < 100) / nrow(pattern_counts)
```
Indeed, less than 20 % of patterns have less than 100 examples.

```{r}
write(capture.output(sessionInfo()), "03_descriptive_stats_sessioninfo.txt")
```
