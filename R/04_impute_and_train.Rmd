---
title: "03: Impute data and train model"
output: html_notebook
---

## Setup
```{r}
library(magrittr)
library(futile.logger)
library(caret)
library(mice)
library(ModelMetrics)
library(DMwR)
library(pcaMethods)

source("imputation_definitions.R")
source("recursive_application.R")
source("imputation.R")
```

```{r}
training_data <- read.csv("../contracted_training_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../training_outcomes.csv", as.is = TRUE)

# Recode outcomes as 1 -> "positive", 0 -> "negative"
outcome <- factor(outcome[,2], levels = c("1", "0"), labels = c("positive", "negative"))
head(outcome)
```

## Removal of problematic features

Some imputation methods cannot deal with features that have certain unwanted properties, and thus they must be removed prior to imputation.

### Near-zero variance

```{r}
nzv_features <- caret::nearZeroVar(training_data, saveMetrics = TRUE)
print(nzv_features[nzv_features$nzv, ])

if (any(nzv_features$nzv)) {
  training_data <- training_data[, !nzv_features$nzv]
}
```

### Highly correlated features

```{r}
correlations <- cor(training_data, use = "pairwise.complete.obs")
correlations[is.na(correlations)] <- 0.0

highly_correlated_features <- caret::findCorrelation(correlations, verbose = TRUE, names = TRUE)
print(highly_correlated_features)

if(highly_correlated_features %>% length > 0) {
  training_data <- training_data[, !colnames(training_data) %in% highly_correlated_features]
}
```

## Imputation

Check the number of hyperparameter configurations for each imputation method:
```{r}
lapply(mice_hyperparameter_grids, nrow)
lapply(other_hyperparameter_grids, nrow)
```

```{r}
times <- 2
iters <- 2
```

### MICE

```{r}

mice_imputations <- lapply(enumerate(mice_hyperparameter_grids), function(method) {

  hyperparams <- method$value

  if(nrow(hyperparams) == 0) {
    imputations <- list(run_mice(training_data, method$name, list(), times, iters))
  }
  else {
  imputations <- lapply(1:nrow(hyperparams),
                        function(x) run_mice(training_data, method$name, unlist(hyperparams[x,]), times, iters))
  }

  # Combine timings of different hyperparameter configs
  timings <- do.call(rbind, lapply(imputations, . %>% attr("timing")))
  # Return them along with the imputation object
  attr(imputations, "timings") <- timings

  return(imputations)
})
```

### Non-MICE imputations

```{r}
other_imputations <- lapply(enumerate(other_hyperparameter_grids), function(method) {

  hyperparams <- method$value

  # TODO Implement timing
  if (method$name == "bpca") {
    imputations <- lapply(1:nrow(hyperparams), function(x) {
      run_bpca(data = training_data, hyperparams = hyperparams[x, ], times = times)
    })
  }
  else if (method$name == "knnImputation") {
    imputations <- lapply(1:nrow(hyperparams), function(x) {
      run_knn(data = training_data, hyperparams = hyperparams[x, ], times = times)
    })
  } else {
    print("Method " %>% paste0(method$name, " is not implemented"))
    imputations <- NULL
  }

  # Combine timings of different hyperparameter configs
  timings <- do.call(rbind, lapply(imputations, . %>% attr("timing")))
  # Return them along with the imputation object
  attr(imputations, "timings") <- timings

  return(imputations)
})
```

### Single value imputations

```{r}
single_value_imputations <- lapply(enumerate(single_value_imputation_hyperparameter_grids), function(method) {
  list(`1` = list(completed_datasets = list(data.frame(get(method$name)(training_data)))))
})
names(single_value_imputations) <- names(single_value_imputation_hyperparameter_grids)
```

### List and drop imputation methods that failed completely

```{r}
imputations <- c(mice_imputations, other_imputations, single_value_imputations)
```

```{r}
valid_methods <- sapply(names(imputations), function(method) {

  null_hpsets <- sapply(imputations[[method]], function(x) x[["completed_datasets"]] %>% unlist %>% is.null)
  if (all(null_hpsets)) {
    print(paste("Imputation method", method, "did not successfully produce any datasets"))
    return(FALSE)
  }
  return(TRUE)
})
imputations <- imputations[valid_methods]
```

## Training classifier
```{r}
hyperparameter_grid <- data.frame(mtry = 1:5 * 8 - 1)

rf_training_settings <- trainControl(classProbs = TRUE,
                                     verboseIter = FALSE,
                                     method = "oob",  # Use out-of-bag error estimate for model selection
                                     returnResamp = "final")
lr_training_settings <- trainControl(classProbs = TRUE,
                                     verboseIter = FALSE)

train_rf <- function(dataset) {

  rf_model <- NULL
  tryCatch({
    rf_model <- caret::train(x = dataset,
                             y = outcome,
                             method = "rf",
                             preProcess = c("center", "scale"),
                             trControl = rf_training_settings,
                             tuneGrid = hyperparameter_grid)
  }, error = function(e) print(e))

  return(rf_model)
}
train_lr <- function(dataset) {

  lr_model <- NULL
  tryCatch({
    lr_model <- caret::train(x = dataset,
                             y = outcome,
                             method = "glm",
                             preProcess = c("center", "scale"),
                             trControl = lr_training_settings)
  }, error = function(e) print(e))

  return(lr_model)
}
# Train on every completed dataset
rf_models <- recursive_apply(x = imputations,
                             fun = train_rf,
                             x_class = "data.frame")
lr_models <- recursive_apply(x = imputations,
                             fun = train_lr,
                             x_class = "data.frame")
```

## Model selection
```{r}
extract_oob_performance <- function(model) {
  model$finalModel$err.rate[, "OOB"] %>% tail(1)
}
extract_mcc_performance <- function(model) {
  ModelMetrics::mcc(as.integer(outcome == "positive"), 1 - model$finalModel$fitted.values, 0.5)
}

# Find index of model with best mean OOB error
inf_NULLs <- function(x, positive = TRUE) {
  x[sapply(x, is.null)] <- ifelse(positive, Inf, -Inf)
  return(x)
}

select_best <- function(models, imputations, hyperparams, performance_function, positive) {
  # Get the error estimate from each leaf of the tree (i.e. all trained models)
  perf <- recursive_apply(models, performance_function, "train")

  # The OOB estimates are in lists with one value per completed dataset.
  # Unlisting that list before mean gives mean the desired input type (numeric vector).
  mean_perf <- leaf_apply(perf, . %>% unlist %>% mean, docall = FALSE)

  # The means are now inside lists with a single component "completed_datasets" which then contains the mean.
  # We can remove that redundant node with unlist.
  mean_perf <- leaf_apply(mean_perf, unlist, docall = FALSE)
  best_model_ix <- leaf_apply(mean_perf, . %>% inf_NULLs(positive = positive) %>% which.min, docall = FALSE)

  # Extract the best models, best imputers and their best hyperparameters for each method
  best_models <- lapply(enumerate(best_model_ix), . %>% with(models[[name]][[value]]))
  best_imputers <- lapply(enumerate(best_model_ix), . %>% with(imputations[[name]][[value]]))
  best_hyperparams <- lapply(enumerate(best_model_ix), . %>% with(hyperparams[[name]][value, ]))

  # We can drop the now superfluous level "completed_datasets" of the tree
  best_models <- lapply(best_models, . %>% extract2(1))

  return(list(ix = best_model_ix, models = best_models, imputers = best_imputers, hyperparams = best_hyperparams))
}

rf_bests <- select_best(rf_models, imputations, c(mice_hyperparameter_grids, other_hyperparameter_grids, single_value_imputation_hyperparameter_grids), performance_function = extract_oob_performance, TRUE)
lr_bests <- select_best(lr_models, imputations, c(mice_hyperparameter_grids, other_hyperparameter_grids, single_value_imputation_hyperparameter_grids), performance_function = extract_mcc_performance, FALSE)
```

## Model diagnostics
```{r}
# Produce convergence plots for imputation methods
rf_imputation_convergence_plots <- lapply(rf_bests$imputers, . %>% recursive_apply(plot, "mids"))
lr_imputation_convergence_plots <- lapply(lr_bests$imputers, . %>% recursive_apply(plot, "mids"))

# Produce convergence plots for random forest classifiers
rf_classifier_oob_plots <- recursive_apply(rf_bests$models, plot, x_class = "train")
```

## Saving model
```{r}
if (!dir.exists("../output")) {
  dir_creation_success <- dir.create("../output", showWarnings = TRUE)
  if (!dir_creation_success) {
    stop("Failed to create directory for saving output.")
  }
}

saveRDS(rf_bests$models, file = "../output/rf_classifiers.rds")
saveRDS(rf_bests$imputers, file = "../output/rf_imputers.rds")
saveRDS(rf_bests$hyperparams, file = "../output/rf_hp_configs.rds")

saveRDS(lr_bests$models, file = "../output/lr_classifiers.rds")
saveRDS(lr_bests$imputers, file = "../output/lr_imputers.rds")
saveRDS(lr_bests$hyperparams, file = "../output/lr_hp_configs.rds")

saveRDS(colnames(training_data), "../output/final_features.rds")
```
