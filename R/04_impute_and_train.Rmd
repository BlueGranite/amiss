---
title: "03: Impute data and train model"
output: html_notebook
---

## Setup
```{r}
library(magrittr)
library(futile.logger)
library(caret)
library(mice)

source("imputation_definitions.R")
source("recursive_application.R")
source("imputation.R")
```

```{r}
training_data <- read.csv("../contracted_training_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../outcomes.csv", as.is = TRUE)

# Recode outcomes as 1 -> "positive", 0 -> "negative"
outcome <- factor(outcome[,2], levels = c("1", "0"), labels = c("positive", "negative"))
head(outcome)
```

## Removal of problematic features

Some imputation methods cannot deal with features that have certain unwanted properties, and thus they must be removed prior to imputation.

### Near-zero variance

```{r}
nzv_features <- caret::nearZeroVar(training_data, saveMetrics = TRUE)
print(nzv_features[nzv_features$nzv, ])

if (any(nzv_features$nzv)) {
  training_data <- training_data[, !nzv_features$nzv]
}
```

### Highly correlated features

```{r}
correlations <- cor(training_data, use = "pairwise.complete.obs")
correlations[is.na(correlations)] <- 0.0

highly_correlated_features <- caret::findCorrelation(correlations, verbose = TRUE, names = TRUE)
print(highly_correlated_features)

if(highly_correlated_features %>% length > 0) {
  training_data <- training_data[, !colnames(training_data) %in% highly_correlated_features]
}
```

## Imputation

Check the number of hyperparameter configurations for each imputation method:
```{r}
lapply(imputation_hyperparameter_grids, nrow)
```

### MICE

```{r}
times <- 2
iters <- 2

imputations <- lapply(enumerate(imputation_hyperparameter_grids), function(method) {

  hyperparams <- method$value

  imputations <- lapply(1:nrow(hyperparams),
                        function(x) run_mice(training_data, method$name, unlist(hyperparams[x,]), times, iters))

  # Combine timings of different hyperparameter configs
  timings <- do.call(rbind, lapply(imputations, . %>% attr("timing")))
  # Return them along with the imputation object
  attr(imputations, "timings") <- timings

  return(imputations)
})
```

### List and drop imputation methods that failed completely
```{r}
valid_methods <- sapply(names(imputations), function(method) {
  
  null_hpsets <- sapply(imputations[[method]], function(x) x[["imputation_object"]] %>% is.null)
  
  if (all(null_hpsets)) {
    print(paste("Imputation method", method, "did not successfully produce any datasets"))
    return(FALSE)
  }
  return(TRUE)
})
imputations <- imputations[valid_methods]
```

## Training classifier
```{r}
hyperparameter_grid <- data.frame(mtry = 1:5 * 8 - 1)

rf_training_settings <- trainControl(classProbs = TRUE,
                                     verboseIter = FALSE,
                                     method = "oob",  # Use out-of-bag error estimate for model selection
                                     returnResamp = "final")
lr_training_settings <- trainControl(classProbs = TRUE,
                                     verboseIter = FALSE)

train_rf <- function(dataset) {

  rf_model <- NULL
  tryCatch({
    rf_model <- caret::train(x = dataset,
                             y = outcome,
                             method = "rf",
                             preProcess = c("center", "scale"),
                             trControl = rf_training_settings,
                             tuneGrid = hyperparameter_grid)
  }, error = function(e) print(e))

  return(rf_model)
}
train_lr <- function(dataset) {

  lr_model <- NULL
  tryCatch({
    lr_model <- caret::train(x = dataset,
                             y = outcome,
                             method = "glm",
                             preProcess = c("center", "scale"),
                             trControl = lr_training_settings)
  }, error = function(e) print(e))

  return(lr_model)
}
# Train on every completed dataset
rf_models <- recursive_apply(x = imputations,
                          fun = train_rf,
                          x_class = "data.frame")
lr_models <- recursive_apply(x = imputations,
                          fun = train_lr,
                          x_class = "data.frame")
```

## Model selection
```{r}
extract_oob_performance <- function(model) {
  model$finalModel$err.rate[, "OOB"] %>% tail(1)
}
library(ModelMetrics)
extract_mcc_performance <- function(model) {
  #pred <- rep("negative", nrow(model$trainingData))
  #pred[model$finalModel$fitted.values > 0.5, "positive"]
  ModelMetrics::mcc(1 - as.integer(outcome == "positive"), model$finalModel$fitted.values, 0.5)
}

# Get the OOB error estimate from each leaf of the tree (i.e. all trained models)
rf_oobs <- recursive_apply(rf_models, extract_oob_performance, "train")
lr_mccs <- recursive_apply(lr_models, extract_mcc_performance, "train")

# The OOB estimates are in lists with one value per completed dataset.
# Unlisting that list before mean gives mean the desired input type (numeric vector).
rf_oob_means <- leaf_apply(rf_oobs, . %>% unlist %>% mean, docall = FALSE)
lr_mcc_means <- leaf_apply(lr_mccs, . %>% unlist %>% mean, docall = FALSE)

# The means are now inside lists with a single component "completed_datasets" which then contains the mean.
# We can remove that redundant node with unlist.
rf_oob_means <- leaf_apply(rf_oob_means, unlist, docall = FALSE)
lr_mcc_means <- leaf_apply(lr_mcc_means, unlist, docall = FALSE)

# Find index of model with best mean OOB error
inf_NULLs <- function(x, positive = TRUE) {
  x[sapply(x, is.null)] <- ifelse(positive, Inf, -Inf)
  return(x)
}
rf_best_model_index <- leaf_apply(rf_oob_means, . %>% inf_NULLs %>% which.min, docall = FALSE)
lr_best_model_index <- leaf_apply(lr_mcc_means, . %>% inf_NULLs(positive = FALSE) %>% which.max, docall = FALSE)

# Extract the best models, best imputers and their best hyperparameters for each method
rf_best_models <- lapply(enumerate(rf_best_model_index), . %>% with(rf_models[[name]][[value]]))
rf_best_models <- lapply(rf_best_models, . %>% extract2(1))

lr_best_models <- lapply(enumerate(lr_best_model_index), . %>% with(lr_models[[name]][[value]]))
lr_best_models <- lapply(lr_best_models, . %>% extract2(1))

rf_best_imputers <- lapply(enumerate(rf_best_model_index), . %>% with(imputations[[name]][[value]]))

lr_best_imputers <- lapply(enumerate(lr_best_model_index), . %>% with(imputations[[name]][[value]]))

rf_best_hyperparams <- lapply(enumerate(rf_best_model_index), . %>% with(imputation_hyperparameter_grids[[name]][value, ]))
lr_best_hyperparams <- lapply(enumerate(lr_best_model_index), . %>% with(imputation_hyperparameter_grids[[name]][value, ]))
```

## Model diagnostics
```{r}
# Produce convergence plots for imputation methods
rf_imputation_convergence_plots <- lapply(rf_best_imputers, . %>% recursive_apply(plot, "mids"))
lr_imputation_convergence_plots <- lapply(lr_best_imputers, . %>% recursive_apply(plot, "mids"))

# Produce convergence plots for random forest classifiers
rf_classifier_oob_plots <- recursive_apply(rf_best_models, plot, x_class = "train")
```

## Saving model
```{r}
if (!dir.exists("../output")) {
  dir_creation_success <- dir.create("../output", showWarnings = TRUE)
  if (!dir_creation_success) {
    stop("Failed to create directory for saving output.")
  }
}

saveRDS(rf_best_models, file = "../output/rf_classifier_models.rds")
saveRDS(rf_best_imputers, file = "../output/rf_imputers_models.rds")
saveRDS(rf_best_hyperparams, file = "../output/rf_hp_configs_models.rds")

saveRDS(lr_best_models, file = "../output/lr_classifier_models.rds")
saveRDS(lr_best_imputers, file = "../output/lr_imputers_models.rds")
saveRDS(lr_best_hyperparams, file = "../output/lr_hp_configs_models.rds")

saveRDS(colnames(training_data), "../output/final_features.rds")
```
