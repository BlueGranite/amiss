---
title: "03: Impute data and train model"
output: html_notebook
---

## Setup
```{r}
library(magrittr)
library(futile.logger)
library(caret)
library(mice)

source("imputation_definitions.R")
source("functionals.R")
```

```{r}
training_data <- read.csv("../contracted_training_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../outcomes.csv", as.is = TRUE)

outcome <- factor(outcome[,2], levels = c("1", "0"), labels = c("positive", "negative"))
head(outcome)
```

## Removal of problematic features

### Near-zero variance
```{r}
nzv_features <- caret::nearZeroVar(training_data, saveMetrics = TRUE)
print(nzv_features[nzv_features$nzv, ])

if (any(nzv_features$nzv)) {
  training_data <- training_data[, !nzv_features$nzv]
}
```

### Highly correlated features
```{r}
correlations <- cor(training_data, use = "pairwise.complete.obs")
correlations[is.na(correlations)] <- 0.0

highly_correlated_features <- caret::findCorrelation(correlations, verbose = TRUE, names = TRUE)
print(highly_correlated_features)

if(highly_correlated_features %>% length > 0) {
  training_data <- training_data[, !colnames(training_data) %in% highly_correlated_features]
}
```

## Imputation

```{r}
lapply(imputation_hyperparameter_grids, nrow)
```

```{r}
run_imputation_method <- function(data, method, hyperparams, times, iterations) {

  print(paste0("Starting with ", paste0(hyperparams, collapse = ", "), ", imputing ", times, " times"))

  imputation_object <- mice::mice(data = data,
                                  method = method,
                                  m = times,
                                  maxit = iterations,
                                  printFlag = FALSE,
                                  ... = hyperparams)

  print("done with imputation")

  completed_datasets <- mice::complete(imputation_object, action = "all")

  print("done with forming datasets")

  return(list(
    completed_datasets = completed_datasets,
    imputation_object = imputation_object
  ))
}

times <- 1
iters <- 1

impute_with_hyperparameters <- function(method, hyperparams) {

  # Named vector of hyperparameters
  imp_args <- list(data = training_data,
                   method = method,
                   iterations = iters,
                   hyperparams = hyperparams,
                   times = times)

  timing <- system.time(

    imputation <- tryCatch(do.call(run_imputation_method, imp_args),
                           error = function(e) {
                             print(e)
                           })
  )
  attr(imputation, "timing") <- timing
  return(imputation)
}

imputations <- lapply(enumerate(imputation_hyperparameter_grids), function(method) {

  hyperparams <- method$value

  imputations <- lapply(1:nrow(hyperparams),
                        function(x) impute_with_hyperparameters(method$name, unlist(hyperparams[x,])))

  timings <- do.call(rbind, lapply(imputations, . %>% attr("timing")))
  attr(imputations, "timings") <- timings

  return(imputations)
})
# lapply(imputations, . %>% attr("timings"))
```

## Training
```{r}
hyperparameter_grid <- data.frame(mtry = 1:5 * 8 - 1)

training_settings <- trainControl(classProbs = TRUE,
                                  verboseIter = FALSE,
                                  method = "oob",
                                  returnResamp = "final")

train_rf <- function(dataset) {

  rf_model <- caret::train(x = dataset,
                           y = outcome,
                           method = "rf",
                           preProcess = c("center", "scale"),
                           trControl = training_settings,
                           tuneGrid = hyperparameter_grid)

  return(rf_model)
}

models <- recursive_apply(x = imputations,
                          fun = train_rf,
                          x_class = "data.frame")
```

```{r}
extract_oob_performance <- function(model) {
  model$finalModel$err.rate[, "OOB"] %>% tail(1)
}

oobs <- recursive_apply(models, extract_oob_performance, "train")

oob_means <- leaf_apply(oobs, . %>% unlist %>% mean, docall = FALSE)

# The tree depth is now one level deeper than it has to be, so we can flatten it with unlist
oob_means <- leaf_apply(oob_means, unlist, docall = FALSE)
best_model_index <- leaf_apply(oob_means, which.min, docall = FALSE)

best_models <- lapply(enumerate(best_model_index), . %>% with(models[[name]][[value]]))
best_imputers <- lapply(enumerate(best_model_index), . %>% with(imputations[[name]][[value]]))
best_hyperparams <- lapply(enumerate(best_model_index), . %>% with(imputation_hyperparameter_grids[[name]][value, ]))

imputation_convergence_plots <- lapply(best_imputers, . %>% recursive_apply(plot, "mids"))

classifier_oob_plots <- recursive_apply(best_models, plot, x_class = "train")
```

## Saving model
```{r}
if (!dir.exists("../output")) {
  dir_creation_success <- dir.create("../output", showWarnings = TRUE)
  if (!dir_creation_success) {
    stop("Failed to create directory for saving output.")
  }
}

saveRDS(best_models, file = "../output/classifier_models.rds")
saveRDS(best_imputers, file = "../output/imputers_models.rds")
saveRDS(best_hyperparams, file = "../output/hp_configs_models.rds")
```
