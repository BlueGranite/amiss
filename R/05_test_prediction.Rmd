---
title: "Prediction on test set"
output: html_notebook
---

## Read data
```{r}
library(magrittr)
library(mice)
library(caret)

source("recursive_application.R")
source("imputation.R")

test_data <- read.csv("../contracted_test_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../test_outcomes.csv", as.is = TRUE)

# Keep exactly those features that were kept in training data
final_features <- readRDS("../output/final_features.rds")
test_data <- test_data[, final_features]

# Recode outcomes as 1 -> "positive", 0 -> "negative"
outcome <- factor(outcome[,2], levels = c("1", "0"), labels = c("positive", "negative"))
head(outcome)
```

## Multiply impute the test set using the best hyperparameter configurations from the training set
```{r}
hyperparams <- readRDS("../output/hp_configs_models.rds")

times <- 5
iters <- 1

# Create a tree of functions with hyperparameter configurations fixed by currying the
# mice function; i.e. create new functions with all arguments besides `method` fixed
imputation_functions <- lapply(hyperparams, FUN = function(hps) {
  function(method) run_mice(test_data, method, hps, times, iters)
})

# Run each function in the tree using the method designated by the subtree
imputations <- lapply(names(imputation_functions),
                      function(method) {
                        recursive_apply(imputation_functions[[method]],
                                        fun = function(x) x(method),
                                        "function")
                      })
names(imputations) <- names(imputation_functions)

# In case some variables were not otherwise imputable on the test set for whatever reason, run median imputation on them.
completions <- recursive_apply(imputations, function(df) lapply(df, median_imp) %>% data.frame, "data.frame")
```

## Predict on test set completions using best classifier models
```{r}
models <- readRDS("../output/classifier_models.rds")

predictions <- lapply(names(models), function(method) {

  pred_per_model <- lapply(models[[method]], function(model) {

    pred_per_completion <- recursive_apply(completions, function(completed_dataset) {
      predict(model, completed_dataset, type = "prob")[,"positive", drop = TRUE]
    }, "data.frame")

    names(pred_per_completion) <- paste0("imp_", 1:length(pred_per_completion))
    pred_per_completion

  })

  names(pred_per_model) <- paste0("model_", 1:length(pred_per_model))
  pred_per_model

})
names(predictions) <- names(models)
```

## Compute performance statistics on the test set
```{r}
confusion_matrices <- recursive_apply(predictions, function(pred) {
  pred <- factor(c("positive", "negative")[2 - (pred > 0.5)], c("positive", "negative"))
  caret::confusionMatrix(pred, outcome)
}, x_class = "numeric")

mccs <- recursive_apply(predictions, function(pred) ModelMetrics::mcc(as.integer(outcome == "positive"), pred, 0.5), x_class = "numeric")
aucs <- recursive_apply(predictions, function(pred) ModelMetrics::auc(as.integer(outcome == "positive"), pred), x_class = "numeric")

mean_mccs <- leaf_apply(mccs, . %>% unlist %>% mean, FALSE)
mean_aucs <- leaf_apply(aucs, . %>% unlist %>% mean, FALSE)

sd_mccs <- leaf_apply(mccs, . %>% unlist %>% sd, FALSE)
sd_aucs <- leaf_apply(aucs, . %>% unlist %>% sd, FALSE)

mean_mean_mccs <- leaf_apply(mean_mccs, . %>% unlist %>% mean, FALSE)
mean_mean_aucs <- leaf_apply(mean_aucs, . %>% unlist %>% mean, FALSE)

sd_mean_mccs <- leaf_apply(mean_mccs, . %>% unlist %>% sd, FALSE)
sd_mean_aucs <- leaf_apply(mean_aucs, . %>% unlist %>% sd, FALSE)
```