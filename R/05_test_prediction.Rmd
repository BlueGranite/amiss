---
title: "Prediction on test set"
output: html_notebook
---

## Read data
```{r}
library(magrittr)
library(mice)
library(caret)
library(stringr)

source("recursive_application.R")
source("imputation_definitions.R")
source("imputation.R")

test_data <- read.csv("../contracted_test_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../test_outcomes.csv", as.is = TRUE)

# Keep exactly those features that were kept in training data
final_features <- readRDS("../output/final_features.rds")
test_data <- test_data[, final_features]

# Recode outcomes as 1 -> "positive", 0 -> "negative"
outcome <- factor(outcome[,2], levels = c("1", "0"), labels = c("positive", "negative"))
head(outcome)
```

## Multiply impute the test set using the best hyperparameter configurations from the training set
```{r}
rf_hyperparams <- readRDS("../output/rf_hp_configs.rds")
lr_hyperparams <- readRDS("../output/lr_hp_configs.rds")

times <- 5
iters <- 1

impute_w_hps <- function(data, hp_tree){

  # Create a tree of functions with hyperparameter configurations fixed by currying the
  # mice function; i.e. create new functions with all arguments besides `method` fixed
  imputation_functions <- lapply(hp_tree, FUN = function(hps) {
    function(method) {
      if (method %in% names(mice_imputation_hyperparameters)) {

        run_mice(data, method, hps, times, iters)

      } else if (method == "bpca") {

        run_bpca(data, hps)

      } else if (method == "knn") {

        run_knn(data, hps)

      } else if (method %in% names(single_value_imputation_hyperparameter_grids)) {

        list(completed_datasets = list(`1` = get(method)(data)))

      }
    }
  })

  # Run each function in the tree using the method designated by the subtree
  imputations <- lapply(names(imputation_functions),
                        function(method) {
                          recursive_apply(imputation_functions[[method]],
                                          fun = function(x) x(method),
                                          "function")
                        })
  names(imputations) <- names(imputation_functions)

  # In case some variables were not otherwise imputable on the test set for whatever reason, run median imputation on them.
  completions <- recursive_apply(imputations, median_imp, "data.frame") %>% lapply(. %>% extract2(1))

  return(completions)
}
rf_completions <- impute_w_hps(test_data, rf_hyperparams)
lr_completions <- impute_w_hps(test_data, lr_hyperparams)
```

## Predict on test set completions using best classifier models
```{r}
rf_models <- readRDS("../output/rf_classifiers.rds")
lr_models <- readRDS("../output/lr_classifiers.rds")

prediction <- function(models, completions) {

  predictions <- lapply(names(models), function(method) {

    pred_per_model <- lapply(models[[method]], function(model) {

      pred_per_completion <- recursive_apply(completions[[method]], function(completed_dataset) {
        tryCatch({
        predict(model, completed_dataset, type = "prob")[,"positive", drop = TRUE]
        }, error = function(e) print(e))
      }, "data.frame")

      names(pred_per_completion) <- paste0("imp_", 1:length(pred_per_completion))
      pred_per_completion

    })

    names(pred_per_model) <- paste0("model_", 1:length(pred_per_model))
    pred_per_model

  })
  names(predictions) <- names(models)
  return(predictions)
}

rf_predictions <- prediction(rf_models, rf_completions)
lr_predictions <- prediction(lr_models, lr_completions)
```

## Compute performance statistics on the test set
```{r}
performance_stats <- function(predictions) {

  confusion_matrices <- recursive_apply(predictions, function(pred) {
    pred <- factor(c("positive", "negative")[2 - (pred > 0.5)], c("positive", "negative"))
    caret::confusionMatrix(pred, outcome)
  }, x_class = "numeric")

  mccs <- recursive_apply(predictions, function(pred) ModelMetrics::mcc(as.integer(outcome == "positive"), pred, 0.5), x_class = "numeric")
  aucs <- recursive_apply(predictions, function(pred) ModelMetrics::auc(as.integer(outcome == "positive"), pred), x_class = "numeric")

  perfs <- list(mccs = mccs,
                aucs = aucs)

  return(perfs)

}
rf_perf <- performance_stats(rf_predictions)
lr_perf <- performance_stats(lr_predictions)
```

```{r}
turn_table <- function(perf_tree) {

  perf_values_names <- perf_tree %>% recursive_apply(function(x, name_list) return(name_list), x_class = "numeric", pass_node_names = TRUE) %>% leaf_apply(. %>% paste0(collapse = ":"), docall = FALSE) %>% unlist(use.names = FALSE)
  perf_values <- perf_tree %>% unlist(use.names = FALSE)
  names(perf_values) <- perf_values_names
  perf_df <- lapply(names(perf_values), function(name) {
    stringr::str_split(string = name, pattern = stringr::fixed(":"), simplify = TRUE)
  })
  perf_df <- data.frame(do.call(rbind, perf_df), value = perf_values)
  colnames(perf_df) <- c("method", "model_index", "test_realization", "value")
  return(perf_df)
}

tables <- list(
  rf_mcc = turn_table(rf_perf$mccs),
  rf_auc = turn_table(rf_perf$aucs),
  lr_mcc = turn_table(lr_perf$mccs),
  lr_auc = turn_table(lr_perf$aucs)
)

perf_table <- Reduce(function(x, y) merge(x, y, by = c("method", "model_index", "test_realization")), tables)
colnames(perf_table) <- c("method", "model_index", "test_realization", names(tables))
perf_table
```

```{r}
perf_stats <- perf_table[, c("rf_mcc", "rf_auc", "lr_mcc", "lr_auc")]
perf_model_mean_table <- aggregate(perf_stats, perf_table["method"], mean)
perf_model_sd_table <- aggregate(perf_stats, perf_table["method"], sd)
perf_over_test_mean_table <- aggregate(perf_stats, perf_table[c("method", "model_index")], mean)
perf_over_test_sd_table <- aggregate(perf_stats, perf_table[c("method", "model_index")], sd)
perf_over_train_mean_table <- aggregate(perf_stats, perf_table[c("method", "test_realization")], mean)
perf_over_train_sd_table <- aggregate(perf_stats, perf_table[c("method", "test_realization")], sd)
```