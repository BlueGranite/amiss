---
title: "Prediction on test set"
output: html_notebook
---

## Read data
```{r}
library(magrittr)
library(mice)
library(caret)
library(stringr)
library(ggplot2)
library(gridExtra)

source("recursive_application.R")
source("imputation_definitions.R")
source("imputation.R")

test_data <- read.csv("../contracted_test_data.csv", row.names = 1, as.is = TRUE)
outcome <- read.csv("../test_outcomes.csv", as.is = TRUE)

# Keep exactly those features that were kept in training data
final_features <- readRDS("../output/final_features.rds")
test_data <- test_data[, final_features]

# Recode outcomes as 1 -> "positive", 0 -> "negative"
outcome <- factor(outcome[,2], levels = c("1", "0"), labels = c("positive", "negative"))
head(outcome)
```

## Multiply impute the test set using the best hyperparameter configurations from the training set
```{r}
rf_hyperparams <- readRDS("../output/rf_hp_configs.rds")
lr_hyperparams <- readRDS("../output/lr_hp_configs.rds")

times <- 5
iters <- 1

impute_w_hps <- function(data, hp_tree){

  # Create a tree of functions with hyperparameter configurations fixed by currying the
  # mice function; i.e. create new functions with all arguments besides `method` fixed
  imputation_functions <- lapply(hp_tree, FUN = function(hps) {
    function(method) {
      if (method %in% names(mice_imputation_hyperparameters)) {

        run_mice(data, method, hps, times, iters)

      } else if (method == "bpca") {

        run_bpca(data, hps)

      } else if (method == "knn") {

        run_knn(data, hps)

      } else if (method %in% names(single_value_imputation_hyperparameter_grids)) {

        list(completed_datasets = list(`1` = get(method)(data)))

      }
    }
  })

  # Run each function in the tree using the method designated by the subtree
  imputations <- lapply(names(imputation_functions),
                        function(method) {
                          recursive_apply(imputation_functions[[method]],
                                          fun = function(x) x(method),
                                          "function")
                        })
  names(imputations) <- names(imputation_functions)

  # In case some variables were not otherwise imputable on the test set for whatever reason, run median imputation on them.
  completions <- recursive_apply(imputations, median_imp, "data.frame") %>% lapply(. %>% extract2(1))

  return(completions)
}
rf_completions <- impute_w_hps(test_data, rf_hyperparams)
lr_completions <- impute_w_hps(test_data, lr_hyperparams)
```

## Predict on test set completions using best classifier models
```{r}
rf_models <- readRDS("../output/rf_classifiers.rds")
lr_models <- readRDS("../output/lr_classifiers.rds")

prediction <- function(models, completions) {

  predictions <- lapply(names(models), function(method) {

    pred_per_model <- lapply(models[[method]], function(model) {

      pred_per_completion <- recursive_apply(completions[[method]], function(completed_dataset) {
        tryCatch({
        predict(model, completed_dataset, type = "prob")[,"positive", drop = TRUE]
        }, error = function(e) print(e))
      }, "data.frame")

      names(pred_per_completion) <- paste0("imp_", 1:length(pred_per_completion))
      pred_per_completion

    })

    names(pred_per_model) <- paste0("model_", 1:length(pred_per_model))
    pred_per_model

  })
  names(predictions) <- names(models)
  return(predictions)
}

rf_predictions <- prediction(rf_models, rf_completions)
lr_predictions <- prediction(lr_models, lr_completions)
```

## Compute performance statistics on the test set
```{r}
performance_stats <- function(predictions) {

  confusion_matrices <- recursive_apply(predictions, function(pred) {
    pred <- factor(c("positive", "negative")[2 - (pred > 0.5)], c("positive", "negative"))
    caret::confusionMatrix(pred, outcome)
  }, x_class = "numeric")

  mcc <- recursive_apply(predictions, function(pred) ModelMetrics::mcc(as.integer(outcome == "positive"), pred, 0.5), x_class = "numeric")
  auc <- recursive_apply(predictions, function(pred) ModelMetrics::auc(as.integer(outcome == "positive"), pred), x_class = "numeric")
  sensitivity <- recursive_apply(confusion_matrices, function(cm) cm$byClass["Sensitivity"], x_class = "confusionMatrix")
  specificity <- recursive_apply(confusion_matrices, function(cm) cm$byClass["Specificity"], x_class = "confusionMatrix")
  f1 <- recursive_apply(confusion_matrices, function(cm) cm$byClass["F1"], x_class = "confusionMatrix")
  precision <- recursive_apply(confusion_matrices, function(cm) cm$byClass["Precision"], x_class = "confusionMatrix")
  recall <- recursive_apply(confusion_matrices, function(cm) cm$byClass["Recall"], x_class = "confusionMatrix")

  perfs <- list(mcc = mcc,
                auc = auc,
                sensitivity = sensitivity,
                specificity = specificity,
                f1 = f1,
                precision = precision,
                recall = recall)

  return(perfs)

}
rf_perf <- performance_stats(rf_predictions)
lr_perf <- performance_stats(lr_predictions)
```

```{r}
turn_table <- function(perf_tree) {

  perf_values_names <- perf_tree %>% recursive_apply(function(x, name_list) return(name_list), x_class = "numeric", pass_node_names = TRUE) %>% leaf_apply(. %>% paste0(collapse = ":"), docall = FALSE) %>% unlist(use.names = FALSE)
  perf_values <- perf_tree %>% unlist(use.names = FALSE)
  names(perf_values) <- perf_values_names
  perf_df <- lapply(names(perf_values), function(name) {
    stringr::str_split(string = name, pattern = stringr::fixed(":"), simplify = TRUE)
  })
  perf_df <- data.frame(do.call(rbind, perf_df), value = perf_values)
  colnames(perf_df) <- c("method", "model_index", "test_realization", "value")
  return(perf_df)
}

rf_tables <- lapply(rf_perf, turn_table)
lr_tables <- lapply(lr_perf, turn_table)

merge_tables <- function(tables) {
  perf_table <- Reduce(function(x, y) merge(x, y, by = c("method", "model_index", "test_realization")), tables)
  colnames(perf_table) <- c("method", "model_index", "test_realization", names(tables))
  perf_table
}
rf_perf_table <- merge_tables(rf_tables)
lr_perf_table <- merge_tables(lr_tables)
rf_perf_table
lr_perf_table
```

```{r}
aggregate_over_perf_table <- function(perf_table) {

  perf_stats <- perf_table[, !colnames(perf_table) %in% c("method", "model_index", "test_realization")]
  return(list(
    model_mean = aggregate(perf_stats, perf_table["method"], mean),
    model_sd = aggregate(perf_stats, perf_table["method"], sd),
    over_test_mean = aggregate(perf_stats, perf_table[c("method", "model_index")], mean),
    over_test_sd = aggregate(perf_stats, perf_table[c("method", "model_index")], sd),
    over_train_mean = aggregate(perf_stats, perf_table[c("method", "test_realization")], mean),
    over_train_sd = aggregate(perf_stats, perf_table[c("method", "test_realization")], sd)
  ))

}
rf_perf_aggregations <- aggregate_over_perf_table(rf_perf_table)
lr_perf_aggregations <- aggregate_over_perf_table(lr_perf_table)
```

```{r}
# RF MCC
grid.arrange(
  ggplot(rf_perf_table, aes(x = method, y = mcc)) + geom_boxplot() + ggtitle("MCC of random forest classifier"),
  ggplot(rf_perf_aggregations$over_test_mean, aes(x = method, y = mcc)) + geom_boxplot() + ggtitle("MCC of random forest classifier", subtitle = "Aggregated over test set realizations"),
  ggplot(rf_perf_aggregations$over_train_mean, aes(x = method, y = mcc)) + geom_boxplot() + ggtitle("MCC of random forest classifier", subtitle = "Aggregated over training set realizations")
)

# RF AUC-ROC
grid.arrange(
  ggplot(rf_perf_table, aes(x = method, y = auc)) + geom_boxplot() + ggtitle("AUC-ROC of random forest classifier"),
  ggplot(rf_perf_aggregations$over_test_mean, aes(x = method, y = auc)) + geom_boxplot() + ggtitle("AUC-ROC of random forest classifier", subtitle = "Aggregated over test set realizations"),
  ggplot(rf_perf_aggregations$over_train_mean, aes(x = method, y = auc)) + geom_boxplot() + ggtitle("AUC-ROC of random forest classifier", subtitle = "Aggregated over training set realizations")
)

# LR MCC
grid.arrange(
  ggplot(lr_perf_table, aes(x = method, y = mcc)) + geom_boxplot() + ggtitle("MCC of logistic regression classifier"),
  ggplot(lr_perf_aggregations$over_test_mean, aes(x = method, y = mcc)) + geom_boxplot() + ggtitle("MCC of logistic regression classifier", subtitle = "Aggregated over test set realizations"),
  ggplot(lr_perf_aggregations$over_train_mean, aes(x = method, y = mcc)) + geom_boxplot() + ggtitle("MCC of logistic regression classifier", subtitle = "Aggregated over training set realizations")
)

# RF AUC-ROC
grid.arrange(
  ggplot(lr_perf_table, aes(x = method, y = auc)) + geom_boxplot() + ggtitle("AUC-ROC of logistic regression classifier"),
  ggplot(lr_perf_aggregations$over_test_mean, aes(x = method, y = auc)) + geom_boxplot() + ggtitle("AUC-ROC of logistic regression classifier", subtitle = "Aggregated over test set realizations"),
  ggplot(lr_perf_aggregations$over_train_mean, aes(x = method, y = auc)) + geom_boxplot() + ggtitle("AUC-ROC of logistic regression classifier", subtitle = "Aggregated over training set realizations")
)
```